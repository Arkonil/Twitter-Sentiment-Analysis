{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#for reprocessing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Dataset and basic data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataframe:  (2896731, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So far, it kinda looks like the world is going...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@cafreeland that was a good strong message to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@EU_Commission @vonderleyen @eucopresident @NA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#JôesWeaknëss -Russia invades Ukraine in large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@DeptEnterprise has published a notice making...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  So far, it kinda looks like the world is going...\n",
       "1  @cafreeland that was a good strong message to ...\n",
       "2  @EU_Commission @vonderleyen @eucopresident @NA...\n",
       "3  #JôesWeaknëss -Russia invades Ukraine in large...\n",
       "4  .@DeptEnterprise has published a notice making..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile('./data/all_data.csv'):\n",
    "    df = pd.read_csv('./data/all_data.csv', index_col=0)\n",
    "else:\n",
    "    from zipfile import ZipFile\n",
    "\n",
    "    dfs = []\n",
    "    for archive in tqdm(os.listdir('./data')):\n",
    "      path_to_archive = os.path.join('./data', archive)\n",
    "      with ZipFile(path_to_archive, 'r') as archive:\n",
    "        for file in archive.filelist:\n",
    "          with archive.open(file.filename, 'r') as fp:\n",
    "            dfs.append(pd.read_csv(fp, lineterminator='\\n'))\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df = df[['content']]\n",
    "    df.content = df.content.str.replace('\\n',' ')\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv('./data/all_data.csv')\n",
    "\n",
    "print('Shape of the dataframe: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Dataframe after removing non-english tweets:  943558\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>far kinda look like world go let russia run ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>russia russia apparently perpetual aggressor i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>russia take capitol ukraine ukraine russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bingo since bachelor ir brazil talk wests faul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>russia pull us iraq difference reason war ukra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  far kinda look like world go let russia run ro...\n",
       "5  russia russia apparently perpetual aggressor i...\n",
       "7         russia take capitol ukraine ukraine russia\n",
       "8  bingo since bachelor ir brazil talk wests faul...\n",
       "9  russia pull us iraq difference reason war ukra..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile('./data/cleaned_data.csv'):\n",
    "    df = pd.read_csv('./data/cleaned_data.csv', index_col=0)\n",
    "else:\n",
    "    # Lower Casing\n",
    "    df['content'] = df.content.str.lower()\n",
    "\n",
    "    # Removal of URLs\n",
    "    def remove_urls(text):\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url_pattern.sub(r'', text)\n",
    "\n",
    "    df['content'] = df.content.apply(lambda x:remove_urls(x))\n",
    "\n",
    "    # Removing Punctuations\n",
    "    def remove_punctuations(text):\n",
    "        return text.translate(str.maketrans('','', string.punctuation))\n",
    "\n",
    "    df['content'] = df.content.apply(lambda x:remove_punctuations(x))\n",
    "\n",
    "    # Removing extra spaces between words if any\n",
    "    df['content'] = df['content'].apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "    # Removing Stopwords\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def remove_stopwords(text):\n",
    "        return ' '.join([words for words in str(text).split() if words not in stop_words])\n",
    "\n",
    "    df['content'] = df['content'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    # Removal of Emojis\n",
    "    def remove_emoji(string):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', string)\n",
    "\n",
    "    df['content'] = df['content'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "    # Removing tweets with non-english words\n",
    "    nltk.download('words')\n",
    "    nltk.download('omw-1.4')\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    to_drop = []\n",
    "\n",
    "    for i in df.index:\n",
    "        for w in nltk.wordpunct_tokenize(df.loc[i, 'content']):\n",
    "            if w not in words or w.isalpha()==False:\n",
    "                to_drop.append(i)\n",
    "                break\n",
    "            break\n",
    "            \n",
    "    df.drop(to_drop,inplace = True)\n",
    "    \n",
    "    # Lemmatization\n",
    "    nltk.download('wordnet')\n",
    "    le = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        return ' '.join([le.lemmatize(word,pos='v') for word in text.split()])\n",
    "\n",
    "    df['content'] = df['content'].apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "    # Saving Cleaned Data\n",
    "    df.to_csv('./data/cleaned_data.csv')\n",
    "\n",
    "print('Length of the Dataframe after removing non-english tweets: ', df.shape[0])\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "120fd1e0c42db0d7e518d8ca1593eb92619584ac715341a4fb60078a2de9f90f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('mva-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
